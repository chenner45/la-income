{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (rdk)",
      "language": "python",
      "name": "rdk"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Framework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zy-f/la-income/blob/feature-netarch/Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLaJAcnbYH6c",
        "outputId": "32c76986-3409-48ba-eb3f-d94a531a0cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# this mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the unzipped\n",
        "# workshop folder, e.g. 'acmlab/workshops/week3'\n",
        "FOLDERNAME = 'acmlab/workshops/project'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/acmlab/workshops/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcDrSI8PTwb5"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWRwmh5XTwb-"
      },
      "source": [
        "from collections import defaultdict, namedtuple\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(229)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import util\n",
        "import webmercator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TjSjWuwSe2e"
      },
      "source": [
        "**Part 1:** The first function you should write is `csv_to_data()`. This should take in a filename and returns a dictionary `data` from zipcode to average income."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIPyFe0CTwcB"
      },
      "source": [
        "def csv_to_data(filename):\n",
        "    \"\"\"Takes in a `filename` and returns a dictionary `data` from zipcode to average income.\n",
        "    For a given row, the average income is computed as \n",
        "    \n",
        "    row['A02650'] / row['N1']\"\"\"\n",
        "    data = None\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCC_FV33SqIU"
      },
      "source": [
        "data = csv_to_data('16zpallnoagi.csv')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4nrYX6sStP9"
      },
      "source": [
        "**Part 2:** you should write the `load_zip_latlon_info` function. This function should:\n",
        "\n",
        "- Take in a filename representing a path to a latitude/longitude data file with the following columns of interest:\n",
        "    - `zip`: the zipcode\n",
        "    - `state`: the state of the zipcode\n",
        "    - `latitude`, `longitude`: the latitude and longitude of the zipcode\n",
        "- Keep only zipcodes that are in our `data` dictionary.\n",
        "- Convert the latitude and longitude to `x, y` values. (The `webmercator.xy(lat, lon, z)` function may be helpful here.)\n",
        "- Create a dictionary from zipcode to `(x, y)` tuples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vF4gS5lTwcE",
        "outputId": "c5262bde-d498-4b01-9814-a8310c641e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "def load_zip_latlon_info(filename):\n",
        "    \"\"\"Takes in a `filename` and returns a dictionary from zipcode to (x, y).\"\"\"\n",
        "    df = pd.read_csv(filename, sep=';')                     # the datafile is separated by semicolons for some reason\n",
        "    df = df[df['state'] == 'CA']                            # will make your code more efficient to only work on CA\n",
        "    zip_to_latlon = {}\n",
        "    for idx, row in df.iterrows():                          # loop through the rows of the dataframe\n",
        "        x, y = None\n",
        "        if (2794 <= x <= 2839) and (6528 <= y <= 6572):     # only zip codes in the data set\n",
        "             zip_to_latlon[row['zip']] = (x, y)             \n",
        "    return zip_to_latlon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataPoint(zipcode=90011, average_income=26.776114732724903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qc_q2xnS3xg"
      },
      "source": [
        "zip_to_latlon = load_zip_latlon_info('ziplatlon.csv')\n",
        "data = dict([kv for kv in data.items() if kv[0] in zip_to_latlon])        # Only keep zipcodes in the "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yehp4gZmTwcH"
      },
      "source": [
        "plt.hist(list(d in data.values()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mggtzgvrVAun"
      },
      "source": [
        "**Part 3:** Write a function `euclidean_distance` that takes in two (x, y) tuples and returns the **squared** Euclidean distance between the two points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ULxIBKTwcJ"
      },
      "source": [
        "def euclidean_distance(pt1, pt2):\n",
        "    return None # fill this out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwqy9FaYS-U"
      },
      "source": [
        "**Part 4**: Write the dataset. The comments should be helpful in walking you through it.\n",
        "\n",
        "For more understanding of how a dataset works, please consult the project handout.\n",
        "\n",
        "Our implementation of the dataset takes about 6 minutes to finish running.  There are definitely more efficient ways to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ6EbaqyTwcL"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, main_dir, transform, data, zip_to_latlon):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "        self.zip_to_latlon = zip_to_latlon\n",
        "        \n",
        "        \n",
        "        self.latlons = []\n",
        "        self.total_imgs = []\n",
        "        oceanic = 0\n",
        "\n",
        "        # This loops through all the images in the directory.\n",
        "        for filename in tqdm(sorted(os.listdir(main_dir))):             # tqdm lets you get a nice progress bar\n",
        "            # Step 1: if the filename is not a .jpg, continue. \n",
        "\n",
        "            # Step 2: extract the x and y values out of the filename.\n",
        "            # Remember that a filename is of the form: 14_2817_6565.jpg \n",
        "            # where x=2817, y=6565.\n",
        "\n",
        "            # Step 3: check if the tile is oceanic (i.e. has elevation 0).\n",
        "            # The util.getElevation function takes in a **latitude** and **longitude** and returns an elevation.\n",
        "            # To get a latitude and longitude from a x and y, use `webmercator.latlon(x, y, z=14)`.\n",
        "            # If it is, ignore it.\n",
        "            \n",
        "            # Step 4: Append (x, y) to the self.latlons list.\n",
        "\n",
        "            image = Image.open('images/' + filename).convert(\"RGB\")\n",
        "            self.total_imgs.append(image)\n",
        "            \n",
        "        self.zipcodes = []                                              # a list of zipcodes \n",
        "        self.tile_to_zipcode = {}\n",
        "        # get the list of zip codes we need\n",
        "        for zipcode, (zipcode_x, zipcode_y) in zip_to_latlon.items():   # loops through zipcodes and their x, y coordinates\n",
        "            for x, y in self.latlons:                                   \n",
        "                if self.in_tile((zipcode_x, zipcode_y), (x, y)):        # check if this zipcode is in the tile\n",
        "                    # print(zipcode, x, y)\n",
        "                    self.zipcodes.add(zipcode)\n",
        "                    self.tile_to_zipcode[(x, y)] = zipcode\n",
        "        \n",
        "        for idx, (x, y) in enumerate(self.latlons):\n",
        "            if (x, y) not in self.tile_to_zipcode:                      # if this tile isn't already assigned\n",
        "                # find the closest zipcode\n",
        "                # you can use the `min(self.zipcodes, key=FUNCTION)` here\n",
        "                best_zipcode = None           # fill this in\n",
        "                self.tile_to_zipcode[(x, y)] = best_zipcode\n",
        "\n",
        "        print(len(self.tile_to_zipcode), \"entries.\")\n",
        "                \n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the dataset—how many images there are, total.\"\"\"\n",
        "        return None\n",
        "    \n",
        "    def coordinates(self, zipcode):\n",
        "        \"\"\"Returns the coordinates of the given zipcode.\"\"\"\n",
        "        return None     # fill this in\n",
        "    \n",
        "    def in_tile(self, coord, square):\n",
        "        \"\"\"checks whether a given coordinate is in a tile\"\"\"\n",
        "        lat, lon = coord\n",
        "        llat, llon = square\n",
        "        ulat, ulon = llat + 1, llon + 1\n",
        "        return (lat >= llat and lon >= llon and lat <= ulat and lon <= ulon)\n",
        "  \n",
        "    def get_image(self, idx):\n",
        "        \"\"\"Returns the image at this index.\"\"\"\n",
        "        return None     # fill this in\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        \"\"\"Given an index, return the ground truth label for that index.\"\"\"\n",
        "        return None     # fill this in\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return the image and label at a given index\"\"\"\n",
        "        tensor_image = None # fill this in\n",
        "        label = None # fill this in\n",
        "        return tensor_image, label\n",
        "    \n",
        "    def display(self, idx):\n",
        "        \"\"\"Displays the image at a given index\"\"\"\n",
        "        display(self.get_image(idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "S3jEPxQsTwcN",
        "outputId": "e48b84ce-20c8-4d4b-a4d0-dd1833411552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "dset = ImageDataset('images', ToTensor(), data, zip_to_latlon)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1981/1981 [06:57<00:00,  4.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "523\n",
            "1457 entries.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_aHM-BGYqZk"
      },
      "source": [
        "This code block creates train and validation dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plXeEHp_TwcQ"
      },
      "source": [
        "validation_split = 0.10\n",
        "dataset_size = len(dset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSLWfEJTwcS"
      },
      "source": [
        "train_dataloader = DataLoader(dset, batch_size=32, sampler=train_sampler)\n",
        "valid_dataloader = DataLoader(dset, batch_size=32, sampler=valid_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T3CbPwKYvLN"
      },
      "source": [
        "**Step 5**: Create the model.  Workshop 3 may be useful for this.\n",
        "We suggest a series of convolutional layers interspersed with `torch.nn.MaxPool2d` layers, followed by a series of linear layers.\n",
        "Each convolutional and linear layer should be followed by an activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4GlbxJjiBT6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBNRelu6(nn.Sequential):\n",
        "    def __init__(self, in_, out, kernel, stride, padding=0, groups=1):\n",
        "        super(ConvBNRelu6, self).__init__(\n",
        "            nn.Conv2d(in_channels=in_, out_channels=out, kernel_size=kernel, stride=stride, padding=padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels=128, c=1, t=1, s=1):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        tk = int(round(in_channels*t))\n",
        "        self.use_res = (s==1) and (in_channels==c)\n",
        "        layers = []\n",
        "        if t != 1:\n",
        "            layers.append(ConvBNRelu6(in_=in_channels, out=tk, kernel=(1,1), stride=(1,1)))\n",
        "        layers += [\n",
        "            ConvBNRelu6(in_=tk, out=tk, kernel=(3,3), stride=(s,s), padding=(1,1), groups=tk), #depthwise\n",
        "            nn.Conv2d(in_channels=tk, out_channels=c, kernel_size=(1,1)),\n",
        "            nn.BatchNorm2d(c)\n",
        "        ]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.use_res:\n",
        "            return x + self.conv(x)\n",
        "        return self.conv(x)\n",
        "\n",
        "# for debug\n",
        "class PrintShape(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintShape, self).__init__()\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJ36pTyKKEG"
      },
      "source": [
        "class Sequence(object):\n",
        "  def __init__(self, **kwargs):\n",
        "      for k,v in kwargs.items():\n",
        "          setattr(self,k,v)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJbEmXlTwcV"
      },
      "source": [
        "# implementing mobilenetv2: https://arxiv.org/pdf/1801.04381.pdf\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_dim=256, img_depth=3, sequence_list=[], dropout=.2, fc_hidden_dims=500):\n",
        "        super(Model, self).__init__()\n",
        "        layers = []\n",
        "        inp_size = img_depth\n",
        "        for seq in sequence_list:\n",
        "          for i in range(seq.n):\n",
        "            if i > 0 and seq.s > 1:\n",
        "                seq.s = 1\n",
        "            if seq.op == 'conv2d':\n",
        "                layers.append(ConvBNRelu6(in_=inp_size, out=seq.c, kernel=seq.kernel, stride=seq.s, padding=seq.kernel//2))\n",
        "            elif seq.op == 'bottleneck':\n",
        "                layers.append(BottleneckBlock(in_channels=inp_size, c=seq.c, t=seq.t, s=seq.s))\n",
        "            elif seq.op == 'avgpool':\n",
        "                layers.append(nn.AvgPool2d(input_dim))\n",
        "            inp_size = seq.c\n",
        "            input_dim //= (input_dim if seq.s==-1 else seq.s)\n",
        "        layers.append(nn.Flatten())\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(inp_size, fc_hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fc_hidden_dims, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.regressor(x)\n",
        "        return x\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DctOln1W5apo",
        "outputId": "b1ee7903-f3f6-4d50-e804-1e1977957e88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# \"unit test\"\n",
        "# ORIGINAL PAPER SETTINGS\n",
        "\n",
        "# for prebuilt model\n",
        "# prebuilt = torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=True)\n",
        "# print(prebuilt)\n",
        "\n",
        "sequence_list = [\n",
        "    Sequence(op='conv2d', t=None, c=32, n=1, s=2, kernel=3),\n",
        "    Sequence(op='bottleneck', t=1, c=16, n=1, s=1),\n",
        "    Sequence(op='bottleneck', t=6, c=24, n=2, s=2),\n",
        "    Sequence(op='bottleneck', t=6, c=32, n=3, s=2),\n",
        "    Sequence(op='bottleneck', t=6, c=64, n=4, s=2),\n",
        "    Sequence(op='bottleneck', t=6, c=96, n=3, s=1),\n",
        "    Sequence(op='bottleneck', t=6, c=160, n=3, s=2),\n",
        "    Sequence(op='bottleneck', t=6, c=320, n=1, s=1),\n",
        "    Sequence(op='conv2d', t=None, c=1280, n=1, s=1, kernel=1),\n",
        "    Sequence(op='avgpool', t=None, c=1280, n=1, s=-1), #input is now just a vertical stack 1x1x1280 -> linear time\n",
        "]\n",
        "\n",
        "model = Model(input_dim=256, img_depth=3, sequence_list=sequence_list).cuda()\n",
        "# print(model)\n",
        "test_batch = torch.rand(8,3,256,256).cuda()\n",
        "out = model(test_batch)\n",
        "print(out)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0361],\n",
            "        [-0.0410],\n",
            "        [-0.0568],\n",
            "        [-0.0819],\n",
            "        [-0.0221],\n",
            "        [-0.0165],\n",
            "        [-0.0443],\n",
            "        [-0.0722]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtDjwL6yaYUJ"
      },
      "source": [
        "**Step 6**: Train the model. Use `torch.nn.MSELoss(reduction='sum')` here, or one of the other losses specified in the project spec, since we're trying to output a real value (not categories). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jF6i0U8TwcX"
      },
      "source": [
        "# train loop\n",
        "model = Model(input_dim=256, img_depth=3, sequence_list=sequence_list)\n",
        "model = model.cuda()\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "model.train()\n",
        "num_epochs = 200\n",
        "for epoch_num in range(num_epochs):\n",
        "    avg_train_loss = 0\n",
        "    avg_train_l1 = 0\n",
        "    for i, (img_batch, label) in enumerate(train_dataloader):\n",
        "        img_batch = img_batch.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        pred = model(img_batch)\n",
        "        loss = criterion(pred, label)\n",
        "        # Step 1: feed your predictions into the model, outputting a variable `pred`\n",
        "        # Step 2: calculate the loss w.r.t the predictions and the labels\n",
        "\n",
        "        avg_train_loss += loss.detach()\n",
        "        avg_train_l1 += torch.abs(pred - label).sum()\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss /= len(train_indices)\n",
        "    avg_train_l1 /= len(train_indices)\n",
        "    print(f'Epoch {epoch_num+1} -> Train loss: {avg_train_loss}    Train L1: {avg_train_l1}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        avg_val_loss = 0\n",
        "        avg_val_l1 = 0\n",
        "        for i, (img_batch, label) in enumerate(valid_dataloader):\n",
        "            img_batch = img_batch.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "            pred = model(img_batch)\n",
        "            loss = criterion(pred, label)\n",
        "            # Step 1: feed your predictions into the model, outputting a variable `pred`\n",
        "            # Step 2: calculate the loss w.r.t the predictions and the labels\n",
        "\n",
        "\n",
        "            avg_val_loss += loss\n",
        "            avg_val_l1 += torch.abs(pred - label).sum()\n",
        "        avg_val_loss /= len(val_indices)\n",
        "        avg_val_l1 /= len(val_indices)\n",
        "        print(f'Eval -> Loss: {avg_train_loss}    L1: {avg_train_l1}')\n",
        "        \n",
        "    print(epoch_num, float(avg_train_loss), float(avg_val_loss), float(avg_train_l1), float(avg_val_l1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "973rGkp4TwcZ"
      },
      "source": [
        "for i in [700, 3, 52, 611, 458, 299]:\n",
        "    display(dset.total_imgs[i])\n",
        "    print(dset[i][1])\n",
        "    print(model(dset[i][0].unsqueeze(0).cuda()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}